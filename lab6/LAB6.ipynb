{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">Aula Prática 06</p>\n",
    "\n",
    "## Objetivos desta aula:\n",
    "\n",
    "    - Extração de Características:\n",
    "        - LBP\n",
    "        - GLCM\n",
    "    - Classificação:\n",
    "        - Árvore\n",
    "        - SVM\n",
    "\n",
    "\n",
    "## Professor: Jefersson dos Santos - jefersson@dcc.ufmg.br\n",
    "### Monitor: Caio Cesar - caiosilva@ufmg.br"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importações necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.transform import rotate\n",
    "from skimage.feature import local_binary_pattern, greycomatrix, greycoprops\n",
    "from skimage import data\n",
    "from skimage.color import label2rgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração de cacrterísticas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LBP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conceito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHOD = 'uniform'\n",
    "plt.rcParams['font.size'] = 9\n",
    "\n",
    "\n",
    "def plot_circle(ax, center, radius, color):\n",
    "    circle = plt.Circle(center, radius, facecolor=color, edgecolor='0.5')\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "\n",
    "def plot_lbp_model(ax, binary_values):\n",
    "    \"\"\"Draw the schematic for a local binary pattern.\"\"\"\n",
    "    # Geometry spec\n",
    "    theta = np.deg2rad(45)\n",
    "    R = 1\n",
    "    r = 0.15\n",
    "    w = 1.5\n",
    "    gray = '0.5'\n",
    "\n",
    "    # Draw the central pixel.\n",
    "    plot_circle(ax, (0, 0), radius=r, color=gray)\n",
    "    # Draw the surrounding pixels.\n",
    "    for i, facecolor in enumerate(binary_values):\n",
    "        x = R * np.cos(i * theta)\n",
    "        y = R * np.sin(i * theta)\n",
    "        plot_circle(ax, (x, y), radius=r, color=str(facecolor))\n",
    "\n",
    "    # Draw the pixel grid.\n",
    "    for x in np.linspace(-w, w, 4):\n",
    "        ax.axvline(x, color=gray)\n",
    "        ax.axhline(x, color=gray)\n",
    "\n",
    "    # Tweak the layout.\n",
    "    ax.axis('image')\n",
    "    ax.axis('off')\n",
    "    size = w + 0.2\n",
    "    ax.set_xlim(-size, size)\n",
    "    ax.set_ylim(-size, size)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(ncols=5, figsize=(7, 2))\n",
    "\n",
    "titles = ['flat', 'flat', 'edge', 'corner', 'non-uniform']\n",
    "\n",
    "binary_patterns = [np.zeros(8),\n",
    "                   np.ones(8),\n",
    "                   np.hstack([np.ones(4), np.zeros(4)]),\n",
    "                   np.hstack([np.zeros(3), np.ones(5)]),\n",
    "                   [1, 0, 0, 1, 1, 1, 0, 0]]\n",
    "\n",
    "for ax, values, name in zip(axes, binary_patterns, titles):\n",
    "    plot_lbp_model(ax, values)\n",
    "    ax.set_title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for LBP\n",
    "radius = 3\n",
    "n_points = 8 * radius\n",
    "\n",
    "\n",
    "def overlay_labels(image, lbp, labels):\n",
    "    mask = np.logical_or.reduce([lbp == each for each in labels])\n",
    "    return label2rgb(mask, image=image, bg_label=0, alpha=0.5)\n",
    "\n",
    "\n",
    "def highlight_bars(bars, indexes):\n",
    "    for i in indexes:\n",
    "        bars[i].set_facecolor('r')\n",
    "\n",
    "\n",
    "image = data.load('brick.png')\n",
    "lbp = local_binary_pattern(image, n_points, radius, METHOD)\n",
    "\n",
    "\n",
    "def hist(ax, lbp):\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    return ax.hist(lbp.ravel(), normed=True, bins=n_bins, range=(0, n_bins),\n",
    "                   facecolor='0.5')\n",
    "\n",
    "\n",
    "# plot histograms of LBP of textures\n",
    "fig, (ax_img, ax_hist) = plt.subplots(nrows=2, ncols=3, figsize=(9, 6))\n",
    "plt.gray()\n",
    "\n",
    "titles = ('edge', 'flat', 'corner')\n",
    "w = width = radius - 1\n",
    "edge_labels = range(n_points // 2 - w, n_points // 2 + w + 1)\n",
    "flat_labels = list(range(0, w + 1)) + list(range(n_points - w, n_points + 2))\n",
    "i_14 = n_points // 4            # 1/4th of the histogram\n",
    "i_34 = 3 * (n_points // 4)      # 3/4th of the histogram\n",
    "corner_labels = (list(range(i_14 - w, i_14 + w + 1)) +\n",
    "                 list(range(i_34 - w, i_34 + w + 1)))\n",
    "\n",
    "label_sets = (edge_labels, flat_labels, corner_labels)\n",
    "\n",
    "for ax, labels in zip(ax_img, label_sets):\n",
    "    ax.imshow(overlay_labels(image, lbp, labels))\n",
    "\n",
    "for ax, labels, name in zip(ax_hist, label_sets, titles):\n",
    "    counts, _, bars = hist(ax, lbp)\n",
    "    highlight_bars(bars, labels)\n",
    "    ax.set_ylim(ymax=np.max(counts[:-1]))\n",
    "    ax.set_xlim(xmax=n_points + 2)\n",
    "    ax.set_title(name)\n",
    "\n",
    "ax_hist[0].set_ylabel('Percentage')\n",
    "for ax in ax_img:\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for LBP\n",
    "radius = 2\n",
    "n_points = 8 * radius\n",
    "\n",
    "\n",
    "def kullback_leibler_divergence(p, q):\n",
    "    p = np.asarray(p)\n",
    "    q = np.asarray(q)\n",
    "    filt = np.logical_and(p != 0, q != 0)\n",
    "    return np.sum(p[filt] * np.log2(p[filt] / q[filt]))\n",
    "\n",
    "\n",
    "def match(refs, img):\n",
    "    best_score = 10\n",
    "    best_name = None\n",
    "    lbp = local_binary_pattern(img, n_points, radius, METHOD)\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    hist, _ = np.histogram(lbp, normed=True, bins=n_bins, range=(0, n_bins))\n",
    "    for name, ref in refs.items():\n",
    "        ref_hist, _ = np.histogram(ref, normed=True, bins=n_bins,\n",
    "                                   range=(0, n_bins))\n",
    "        score = kullback_leibler_divergence(hist, ref_hist)\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_name = name\n",
    "    return best_name\n",
    "\n",
    "\n",
    "brick = data.load('brick.png')\n",
    "grass = data.load('grass.png')\n",
    "wall = data.load('rough-wall.png')\n",
    "\n",
    "refs = {\n",
    "    'brick': local_binary_pattern(brick, n_points, radius, METHOD),\n",
    "    'grass': local_binary_pattern(grass, n_points, radius, METHOD),\n",
    "    'wall': local_binary_pattern(wall, n_points, radius, METHOD)\n",
    "}\n",
    "\n",
    "# classify rotated textures\n",
    "print('Rotated images matched against references using LBP:')\n",
    "print('original: brick, rotated: 30deg, match result: ',\n",
    "      match(refs, rotate(brick, angle=30, resize=False)))\n",
    "print('original: brick, rotated: 70deg, match result: ',\n",
    "      match(refs, rotate(brick, angle=70, resize=False)))\n",
    "print('original: grass, rotated: 145deg, match result: ',\n",
    "      match(refs, rotate(grass, angle=145, resize=False)))\n",
    "\n",
    "# plot histograms of LBP of textures\n",
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(nrows=2, ncols=3,\n",
    "                                                       figsize=(9, 6))\n",
    "plt.gray()\n",
    "\n",
    "ax1.imshow(brick)\n",
    "ax1.axis('off')\n",
    "hist(ax4, refs['brick'])\n",
    "ax4.set_ylabel('Percentage')\n",
    "\n",
    "ax2.imshow(grass)\n",
    "ax2.axis('off')\n",
    "hist(ax5, refs['grass'])\n",
    "ax5.set_xlabel('Uniform LBP values')\n",
    "\n",
    "ax3.imshow(wall)\n",
    "ax3.axis('off')\n",
    "hist(ax6, refs['wall'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCLM (Haralick Features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "PATCH_SIZE = 21\n",
    "\n",
    "# open the camera image\n",
    "image = data.camera()\n",
    "\n",
    "# select some patches from grassy areas of the image\n",
    "grass_locations = [(474, 291), (440, 433), (466, 18), (462, 236)]\n",
    "grass_patches = []\n",
    "for loc in grass_locations:\n",
    "    grass_patches.append(image[loc[0]:loc[0] + PATCH_SIZE,\n",
    "                               loc[1]:loc[1] + PATCH_SIZE])\n",
    "\n",
    "# select some patches from sky areas of the image\n",
    "sky_locations = [(54, 48), (21, 233), (90, 380), (195, 330)]\n",
    "sky_patches = []\n",
    "for loc in sky_locations:\n",
    "    sky_patches.append(image[loc[0]:loc[0] + PATCH_SIZE,\n",
    "                             loc[1]:loc[1] + PATCH_SIZE])\n",
    "\n",
    "# compute some GLCM properties each patch\n",
    "xs = []\n",
    "ys = []\n",
    "for patch in (grass_patches + sky_patches):\n",
    "    glcm = greycomatrix(patch, [5], [0], 256, symmetric=True, normed=True)\n",
    "    xs.append(greycoprops(glcm, 'dissimilarity')[0, 0])\n",
    "    ys.append(greycoprops(glcm, 'correlation')[0, 0])\n",
    "\n",
    "# create the figure\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "# display original image with locations of patches\n",
    "ax = fig.add_subplot(3, 2, 1)\n",
    "ax.imshow(image, cmap=plt.cm.gray, interpolation='nearest',\n",
    "          vmin=0, vmax=255)\n",
    "for (y, x) in grass_locations:\n",
    "    ax.plot(x + PATCH_SIZE / 2, y + PATCH_SIZE / 2, 'gs')\n",
    "for (y, x) in sky_locations:\n",
    "    ax.plot(x + PATCH_SIZE / 2, y + PATCH_SIZE / 2, 'bs')\n",
    "ax.set_xlabel('Original Image')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.axis('image')\n",
    "\n",
    "# for each patch, plot (dissimilarity, correlation)\n",
    "ax = fig.add_subplot(3, 2, 2)\n",
    "ax.plot(xs[:len(grass_patches)], ys[:len(grass_patches)], 'go',\n",
    "        label='Grass')\n",
    "ax.plot(xs[len(grass_patches):], ys[len(grass_patches):], 'bo',\n",
    "        label='Sky')\n",
    "ax.set_xlabel('GLCM Dissimilarity')\n",
    "ax.set_ylabel('GLCM Correlation')\n",
    "ax.legend()\n",
    "\n",
    "# display the image patches\n",
    "for i, patch in enumerate(grass_patches):\n",
    "    ax = fig.add_subplot(3, len(grass_patches), len(grass_patches)*1 + i + 1)\n",
    "    ax.imshow(patch, cmap=plt.cm.gray, interpolation='nearest',\n",
    "              vmin=0, vmax=255)\n",
    "    ax.set_xlabel('Grass %d' % (i + 1))\n",
    "\n",
    "for i, patch in enumerate(sky_patches):\n",
    "    ax = fig.add_subplot(3, len(sky_patches), len(sky_patches)*2 + i + 1)\n",
    "    ax.imshow(patch, cmap=plt.cm.gray, interpolation='nearest',\n",
    "              vmin=0, vmax=255)\n",
    "    ax.set_xlabel('Sky %d' % (i + 1))\n",
    "\n",
    "\n",
    "# display the patches and plot\n",
    "fig.suptitle('Grey level co-occurrence matrix features', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação Supervisionada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árvore de Decisão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importações necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score # More metrics here: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando o conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Iris dataset. More information here: http://en.wikipedia.org/wiki/Iris_flower_data_set\n",
    "iris = load_iris()\n",
    "\n",
    "# Printing some informations about the dataset It has 150 instances equally divided into 3 possible classes. In this case, the iris.data has instances from 0 to 49 being from classe 0, instances from 50 to 99 from class 1 and the remaining from class 3.\n",
    "print (iris.feature_names)\n",
    "print (iris.data)\n",
    "\n",
    "print (iris.target_names) # This is the possible classes\n",
    "print (iris.target) # This is the groundtruth\n",
    "\n",
    "print (type(iris.data)) # Type of a variable\n",
    "print (iris.data.shape) # Number of lines x number of columns (150x4, in this case). Shape function can only be used with numpy array type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a DT with default parameters.\n",
    "# More about the parameters here: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "clf_1 = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the DT using the whole dataset using DT. \n",
    "clf_1.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the class of the first instances. The first argument :1 represents the number of lines to consider starting from the first one. In this case, we consider only one line. The second argument symbolizes that it has to consider all columns.\n",
    "print (clf_1.predict(iris.data[:1, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifying all dataset\n",
    "prediction = clf_1.predict(iris.data)\n",
    "# Calculating the accuracy of the prediction and the groundtruth, which is 1 in this case, since the dataset is small and was used as training.\n",
    "accuracy = accuracy_score(iris.target, prediction)\n",
    "print (accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Iris dataset using 40% for test\n",
    "train, test, train_target, test_target = cross_validation.train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\n",
    "\n",
    "# Creating a new DT.\n",
    "clf_2 = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Fitting some training samples. \n",
    "clf_2.fit(train, train_target)\n",
    "\n",
    "# Classifying test samples\n",
    "prediction_2 = clf_2.predict(test)\n",
    "# Calculating the accuracy of the prediction and the groundtruth. Of course, it is not equal one in this case, but very close, since the dataset is small.\n",
    "accuracy_2 = accuracy_score(test_target, prediction_2)\n",
    "print (accuracy_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importações necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import grid_search\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Trains a simple classifier (eg.: SVMs, DT, ...)\n",
    "'''\n",
    "def Simple_Classifier_Function(train, target, test, test_target, classifier, clf_name):\n",
    "    #Start Simple Classifier\n",
    "\n",
    "    classifier.fit(train, target)\n",
    "\n",
    "    prediction = classifier.predict(test)\n",
    "    accuracy = accuracy_score(test_target, prediction)\n",
    "\n",
    "    print ('%s Accuracy %.2f' % (clf_name, accuracy))\n",
    "    return accuracy\n",
    "\n",
    "'''\n",
    "    Search for the best set of parameters for a determined classifier. In this case, it evaluates only two parameters: C (penalty parameter) and gamma (kernel coefficient).\n",
    "    These are fundamental for the SVM-RBF classifier.\n",
    "'''\n",
    "def Simple_Classifier_Function_Grid_Search(train, target, classifier, C, gamma):\n",
    "    #Start Classifier Grid Search\n",
    "\n",
    "    # Parameters to be evaluated\n",
    "    parameters = {'C': C, 'gamma': gamma}\n",
    "\n",
    "    # Searching parameters with 6 cores. Verbose prints in screen at each iteration (higher value, more information).\n",
    "    clf = grid_search.GridSearchCV(classifier, param_grid=parameters, verbose=1, n_jobs=6)\n",
    "\n",
    "    clf.fit(train, target)\n",
    "    print (clf.best_score_)\n",
    "    print (clf.best_params_)\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo Binário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Simple Example')\n",
    "\n",
    "train = [[0, 0], [1, 2], [1, 0], [0, 2], [0, 1], [1, 2]]\n",
    "train_target = [0, 1, 1, 0, 0, 1]\n",
    "\n",
    "test = [[2, 2]]\n",
    "test_target = [1]\n",
    "\n",
    "Simple_Classifier_Function(train, train_target, test, test_target, svm.SVC(), 'SVM-RBF') # SVM-RBF, which is the default configuration of the svm.SVC()\n",
    "Simple_Classifier_Function(train, train_target, test, test_target, svm.LinearSVC(),'Linear SVM') # Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Iris Dataset')\n",
    "\n",
    "# Loading dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Creating 5-folds (cv parameter) with the iris dataset and training a SVM-RBF\n",
    "scores = cross_validation.cross_val_score(svm.SVC(), iris.data, iris.target, cv=5)\n",
    "print ('SVM-RBF accuracy for 5-fold', scores.mean())\n",
    "\n",
    "       \n",
    "       \n",
    "# Checando manualmente\n",
    "print ('\\nSVM accuracy for each fold')\n",
    "\n",
    "# Create the folds (5, in this case). This function returns indices to split data in train test sets.\n",
    "kf = cross_validation.StratifiedKFold(iris.target, n_folds=5)\n",
    "\n",
    "scores_SVMRBF = 0\n",
    "scores_LinearSVM = 0\n",
    "\n",
    "fold = 1\n",
    "for train, test in kf:\n",
    "    print (\"-------------------> Fold %d\" % fold)\n",
    "    fold+=1\n",
    "\n",
    "    # Using indices returned to separate the folds\n",
    "    fold_train = [iris.data[i] for i in train]\n",
    "    fold_target = [iris.target[i] for i in train]\n",
    "    fold_train_test = [iris.data[i] for i in test]\n",
    "    fold_target_test = [iris.target[i] for i in test]\n",
    "\n",
    "    scores_SVMRBF = scores_SVMRBF + Simple_Classifier_Function(fold_train, fold_target, fold_train_test, fold_target_test, svm.SVC(), 'SVM-RBF')\n",
    "    scores_LinearSVM = scores_LinearSVM + Simple_Classifier_Function(fold_train, fold_target, fold_train_test, fold_target_test, svm.LinearSVC(), 'Linear SVM')\n",
    "\n",
    "print ('\\nFinal accuracy')\n",
    "print ('SVM-RBF accuracy', scores_SVMRBF/5.0)\n",
    "print ('Linear SVM accuracy', scores_LinearSVM/5.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo com dígitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Digits Dataset')\n",
    "\n",
    "# Loading dataset. More about this dataset here: http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits\n",
    "digits = load_digits()\n",
    "\n",
    "# Creating 5-folds (cv parameter) with the DIGITS dataset and training a SVM-RBF. The parameter kernel is optional in this case, since it is the default case.\n",
    "scores = cross_validation.cross_val_score(svm.SVC(kernel='rbf'), digits.data, digits.target, cv=5)\n",
    "print ('Default SVM-RBF accuracy for 5-fold', scores.mean()) # 0.44878\n",
    "\n",
    "# For all SVM-RBF, two parameters are fundamental: C (penalty parameter) and gamma (kernel coefficient).\n",
    "# Lets search this parameters.\n",
    "# For the penalty parameter C\n",
    "penalty = np.logspace(-2, 10, 13)\n",
    "print ('Penalty parameter', penalty)\n",
    "# For the coefficient parameter gamma\n",
    "coef = np.logspace(-9, 3, 13)\n",
    "print ('Kernel coefficient', coef)\n",
    "\n",
    "best_clf = Simple_Classifier_Function_Grid_Search(digits.data, digits.target, svm.SVC(kernel='rbf'), C=penalty, gamma=coef)\n",
    "scores = cross_validation.cross_val_score(svm.SVC(kernel='rbf', C=best_clf.best_params_['C'], gamma=best_clf.best_params_['gamma']), digits.data, digits.target, cv=5)\n",
    "print ('Tuned SVM-RBF accuracy for 5-fold', scores.mean()) #Improved results 0.9727\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício\n",
    "\n",
    "    Crie um novo notebook jupyter cujo nome deve seguir a seguinte syntaxe:\n",
    "    \n",
    "        lab6-SUAMATRICULA.ypnb\n",
    "        \n",
    "    Carregar as imagens do conjunto de dados Texturas.\n",
    "    Extrair as características (GLCM e LBP) dos daods.\n",
    "    Aplicar o esquema de validação cruzada na Árvore de Decisão e SVM.\n",
    "    Responder as seguintes perguntas:\n",
    "        (1) qual descritor apresentou os melhores resultados? \n",
    "        (2) qual abordagem de aprendizado apresentou os melhores resultados?\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
