{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Universidade Federal de Minas Gerais </center>\n",
    "<center> Departamento de Ciencia da Computação </center>\n",
    "<center> DCC029/868 – Processamento de Imagens Digitais </center>\n",
    "<center> Prof. Jefersson Alex dos Santos (jefersson@dcc.ufmg.br) </center>\n",
    "<center> Monitor Caio Cesar Viana da Silva (caiosilva@ufmg.br) </center>\n",
    "\n",
    "<h1 align=\"center\">TP2: Classificação de Imagens com Banco de Filtros</h1>\n",
    "\n",
    "<h4 align=\"center\">Alison de Oliveira Souza - 2012049316</h4>\n",
    "<h4 align=\"center\">Yuri Diego Santos Niitsuma - 2011039023</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Documentação</center>\n",
    "\n",
    "#### <center>Introdução:</center>\n",
    "Neste trabalho, abordamos a classificação de imagens de um subconjunto do dataset MNIST - que contém apenas os números 3 e 6 - à partir da definição e da aplicação de um conjunto de filtros de imagens. Esses filtros são construídos utilizando uma rede neural, que simulará filtros aleatórios, inicialmente aplicados em uma parte do subconjunto citado - no nosso caso será de 80% do conjunto inicial, que chamamos de subconjunto de treino - para que sejam obtidas características que nos possibilitem atribuir as imagens a alguma das classes obtidas durante o treino. Após isso, utilizamos a outra parte do subconjunto inicial - os 20% restante que chamamos de subconjunto de teste - para validar nosso banco de filtros de extração de características, utilizando um classificador para tentar identificar a qual classe pertence uma imagem do subconjunto de teste, ou seja, qual número está representado nesta imagem.\n",
    "\n",
    "A imagem abaixo mostra um exemplo do funcionamento de nossa rede neural utilizada para o treinamento.\n",
    "\n",
    "![](image.png)\n",
    "\n",
    "#### <center>Metodologia:</center>\n",
    "Para facilitar o entendimento do nosso projeto, dividimos nosso trabalho em partes. Ao longo desse notebook iremos adicionar células markdown com pedaços da nossa documentação entre cada parte do código, facilitando a leitura e a atribuição de cada etapa da documentação a seu respectivo código.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import das bibliotecas utilizadas e definição de funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from skimage import io, img_as_float\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Função auxiliar para exibir as imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img, cmap=None):\n",
    "    cmap = cmap or plt.cm.gray\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    ax.imshow(img, cmap=cmap)\n",
    "    ax.set_axis_off()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Etapa 1: Funções de leitura do conjunto de dados fornecido\n",
    "As descrições de cada função se encontra como comentários no início de cada uma delas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FOLDER = './MNIST/'\n",
    "SHAPE = (28, 28)\n",
    "\n",
    "def get_files_paths(number):\n",
    "    '''\n",
    "    Retorna todos os arquivos de uma determinada pasta\n",
    "    MNIST/<number>.\n",
    "    '''\n",
    "    files = os.listdir(DATASET_FOLDER + str(number))\n",
    "    for i in range(len(files)):\n",
    "        files[i] = DATASET_FOLDER + str(number) + '/' + files[i]\n",
    "    return files\n",
    "\n",
    "\n",
    "def open_image(filepath=None, as_gray=True):\n",
    "    \"\"\"\n",
    "    Abre a imagem e retorna em formato float.\n",
    "    \"\"\"\n",
    "    if filepath is None:\n",
    "        raise Exception('No filepath passed in open_image!')\n",
    "    return img_as_float(io.imread(filepath, as_gray=as_gray))\n",
    "\n",
    "\n",
    "def load_data(numbers=[]):\n",
    "    \"\"\"\n",
    "    Constrói as listas de imagens e de informações sobre cada\n",
    "    imagem.\n",
    "    data --> lista de imagens\n",
    "    data_id --> lista de classes das imagens (target) \n",
    "    val_id --> lista de posições de cada imagem\n",
    "    \"\"\"\n",
    "    size = len(numbers)\n",
    "    data = list()\n",
    "    data_id = list()\n",
    "    val_id = list()\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for number in numbers:\n",
    "        filename_list = get_files_paths(number)\n",
    "        for filename in filename_list:\n",
    "            data.append(open_image(filename))\n",
    "            an_array = np.zeros((size, 1), dtype=float)\n",
    "            val_id.append(count)\n",
    "            an_array[count] = 1.0\n",
    "            data_id.append(an_array)\n",
    "        \n",
    "        count += 1\n",
    "\n",
    "    return (data, data_id, val_id)\n",
    "\n",
    "\n",
    "def load_data_wrapper(numbers=[]):\n",
    "    \"\"\"\n",
    "    Gera as listas criadas por load_data() e faz uma formatação\n",
    "    simples dos dados.\n",
    "    \"\"\"\n",
    "    samples_inputs, samples_results, samples_validation = load_data(numbers=numbers)\n",
    "    samples_inputs = [np.reshape(x, (784, 1)) for x in samples_inputs]    \n",
    "    samples_data = list(zip(samples_inputs, samples_results, samples_validation))\n",
    "    return samples_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Funções auxiliares.\n",
    "Aqui são declaradas funções extras que são utilizadas na classe Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Função sigmoid. É utilizada como normalizador para o\n",
    "    perceptron. Para saber mais acesse:\n",
    "    https://pt.wikipedia.org/wiki/Fun%C3%A7%C3%A3o_sigm%C3%B3ide\n",
    "    \"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Definição da classe Network\n",
    "Nesta classe, definimos a rede convolucional utilizada pelo nosso projeto. Para isso, utilizamos um algoritmo de aprendizado com gradiente descendente estocástico em uma rede neural. Os gradientes são calculados utilizando backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def show_heights(self):\n",
    "        \"\"\"\n",
    "        Exibe a combinação dos filtros utilizados.\n",
    "        Inicialmente, começa com pesos aleatórios.\n",
    "        \"\"\"\n",
    "        for layer in self.weights:\n",
    "            # print('Shape:' + str(layer.shape))\n",
    "            show(layer)\n",
    "            # for each_neuron in layer:\n",
    "            #     show(each_neuron)\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, data, epochs, mini_batch_size, eta):\n",
    "        \"\"\"\n",
    "        Método que utiliza o gradiente descendente estocástico\n",
    "        para fazer o treinamento da rede neural e classificação das\n",
    "        imagens. Os gradientes são calculados e os pesos das conexões\n",
    "        são atualizadas utilizando backpropagation. \n",
    "        Neste método é feito a separação dos conjuntos de treino e de\n",
    "        testes, o treinamento de acordo com o conjunto de treino\n",
    "        escolhido e a classificação do conjunto de testes.\n",
    "        \"\"\"\n",
    "        print('Total of ' + str(len(data)) + ' images for dataset!')\n",
    "        random.shuffle(data)\n",
    "        # Pega 80% do dataset pra utilizar em treinamento, o restante é\n",
    "        # utilizado nos testes.\n",
    "        n = int(0.8 * len(data))\n",
    "        training_data, test_data = data[:n], data[n:]\n",
    "\n",
    "        n_test = len(test_data)\n",
    "        for j in range(epochs):\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                # print('Efetuando treinamento!')\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                evaluate = self.evaluate(test_data)\n",
    "                print(\"Iteração \" + str(j) + \": \" +\n",
    "                      str(self.evaluate(test_data)) + \" / \" + str(n_test))\n",
    "                print('\\tAcurácia: ' + str(format(evaluate/n_test * 100, '.2f')) + '%')\n",
    "            else:\n",
    "                print(\"Epoch \" + str(j) + \" complete\")\n",
    "            # Atualiza as listas de treino e teste.\n",
    "            random.shuffle(data)\n",
    "            n = int(0.8 * len(data))\n",
    "            training_data, test_data = data[:n], data[n:]\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"\n",
    "        Atualiza os pesos e biases da rede aplicando o gradiente\n",
    "        descendente estocástico usando backpropagation.\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y, z in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "        Retorna uma tupla que representa o gradiente para\n",
    "        a função de custo.\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x]  # list to store all the activations, layer by layer\n",
    "        zs = []  # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"\n",
    "        Retorna o número de entradas de teste para as quais\n",
    "        a rede neural produz o resultado correto.\n",
    "        \"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), z)\n",
    "                        for (x, y, z) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Aplicação da rede neural.\n",
    "Neste bloco, estamos aplicando as funções definidas acima para o treinamento.\n",
    "\n",
    "Inicialmente, delimitamos os conjuntos de dados que serão lidos - no nosso caso os números 3 e 6 - e carregamos as imagens correspondentes.\n",
    "\n",
    "Após isso, definimos a quantidade de neurônios que usaremos por camada. Decidimos por 28x28 neurônios em duas camadas, sendo fully-connected - ou seja, todos se conectam - para que cada um dos neurônios sirva de filtro para cada um dos pixels das imagens de entrada.\n",
    "\n",
    "Depois, inicializamos a rede convolucional e chamamos a função SGD que começa realizando o treinamento da rede neural e depois faz a classificação do conjunto de testes. Nesta função, é efetuada as iterações, passando como parâmetro os dados com os labels do dataset, a quantidade de iterações, a quantidade parcial para utilizar no treinamento, e o parâmetro de peso na aprendizagem - ou seja - a magnitude do deslocamento no domínio dado pelo gradiente.\n",
    "\n",
    "A cada iteração, essa função escolhe aleatoriamente 80% dos dados de entrada para serem os dados de treino e 20% para serem os dados de testes.\n",
    "\n",
    "Ao final de cada iteração, a função irá imprimir a acurácia de cada iteração e irá ajustar a rede neural para que a próxima iteração seja otimizada através do aprendizado obtido. Dessa forma, a tendência é que a cada iteração a acurácia melhore. Porém, algumas variações podem ocorrer devido a escolha aleatória dos conjuntos de treino e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Determina quais são os conjuntos que serão lidos\n",
    "NUMBER_SETS=[3,6]\n",
    "\n",
    "# Carrega as imagens de entrada.\n",
    "data = load_data_wrapper(numbers=NUMBER_SETS)\n",
    "\n",
    "# Determina quantos neurônios estará no layer\n",
    "layers = [28, 28]\n",
    "\n",
    "# Inicializa a rede convulocional\n",
    "net = Network([784, *layers, len(NUMBER_SETS)])\n",
    "net.show_heights()\n",
    "\n",
    "# Efetua as iterações de treinamento e classificação (15 neste caso)\n",
    "net.SGD(data, 15, 10, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
